{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40623, 11)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('2020-12-31-DynamicallyGeneratedHateDataset-entries-v0.1.csv')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0            0\n",
       "id                    0\n",
       "text                  0\n",
       "label                 0\n",
       "type                  0\n",
       "model_wrong       14526\n",
       "db.model_preds    14526\n",
       "status                0\n",
       "round                 0\n",
       "split                 0\n",
       "annotator             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26097, 11)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hate       15253\n",
       "nothate    10844\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>type</th>\n",
       "      <th>model_wrong</th>\n",
       "      <th>db.model_preds</th>\n",
       "      <th>status</th>\n",
       "      <th>round</th>\n",
       "      <th>split</th>\n",
       "      <th>annotator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>It should be legal for a husband to rape his wife</td>\n",
       "      <td>hate</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>True</td>\n",
       "      <td>0.97457</td>\n",
       "      <td>dynabench entry</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>gnzuctwed3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>10001</td>\n",
       "      <td>dalits are lowlife scum</td>\n",
       "      <td>hate</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>False</td>\n",
       "      <td>0.08233</td>\n",
       "      <td>dynabench entry</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>cagnluiznm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     id                                               text label  \\\n",
       "0           1  10000  It should be legal for a husband to rape his wife  hate   \n",
       "1           2  10001                            dalits are lowlife scum  hate   \n",
       "\n",
       "       type model_wrong  db.model_preds           status round  split  \\\n",
       "0  notgiven        True         0.97457  dynabench entry     1  train   \n",
       "1  notgiven       False         0.08233  dynabench entry     1   test   \n",
       "\n",
       "    annotator  \n",
       "0  gnzuctwed3  \n",
       "1  cagnluiznm  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond1a = df['label'] == 'hate'\n",
    "cond1b = df['model_wrong'] == False\n",
    "\n",
    "cond2a = df['label'] == 'nothate'\n",
    "cond2b = df['model_wrong'] == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AL_found'] = np.where((cond1a & cond1b)|\n",
    "                          (cond2a & cond2b),\n",
    "                           True,\n",
    "                           False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>type</th>\n",
       "      <th>model_wrong</th>\n",
       "      <th>db.model_preds</th>\n",
       "      <th>status</th>\n",
       "      <th>round</th>\n",
       "      <th>split</th>\n",
       "      <th>annotator</th>\n",
       "      <th>AL_found</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>It should be legal for a husband to rape his wife</td>\n",
       "      <td>hate</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>True</td>\n",
       "      <td>0.97457</td>\n",
       "      <td>dynabench entry</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>gnzuctwed3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>10001</td>\n",
       "      <td>dalits are lowlife scum</td>\n",
       "      <td>hate</td>\n",
       "      <td>notgiven</td>\n",
       "      <td>False</td>\n",
       "      <td>0.08233</td>\n",
       "      <td>dynabench entry</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>cagnluiznm</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     id                                               text label  \\\n",
       "0           1  10000  It should be legal for a husband to rape his wife  hate   \n",
       "1           2  10001                            dalits are lowlife scum  hate   \n",
       "\n",
       "       type model_wrong  db.model_preds           status round  split  \\\n",
       "0  notgiven        True         0.97457  dynabench entry     1  train   \n",
       "1  notgiven       False         0.08233  dynabench entry     1   test   \n",
       "\n",
       "    annotator  AL_found  \n",
       "0  gnzuctwed3     False  \n",
       "1  cagnluiznm      True  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    15032\n",
       "True     11065\n",
       "Name: AL_found, dtype: int64"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['AL_found'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11065"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "len(df[cond2a & cond2b]) + len(df[cond1a & cond1b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n",
    "df = df.drop(columns=['Unnamed: 0', 'id', 'type', 'model_wrong',\n",
    "       'db.model_preds', 'status', 'round', 'split', 'annotator','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>AL_found</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It should be legal for a husband to rape his wife</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dalits are lowlife scum</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dalits are lowlives</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It was a better world when women dare not ques...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I don't work this hard so that those immigrant...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  AL_found\n",
       "0  It should be legal for a husband to rape his wife     False\n",
       "1                            dalits are lowlife scum      True\n",
       "2                                dalits are lowlives     False\n",
       "3  It was a better world when women dare not ques...     False\n",
       "4  I don't work this hard so that those immigrant...     False"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "416263"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].apply(lambda x: len(x.split(' '))).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_tags=[True, False]\n",
    "plt.figure(figsize=(10,4))\n",
    "df.AL_found.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dalits are lowlife scum\n",
      "\n",
      "\n",
      "I don't work this hard so that all those pakis can take the benefits\n",
      "\n",
      "\n",
      "I don't work this hard so that all those muslims can take the benefits\n",
      "\n",
      "\n",
      "Why is it that Africans smell weird? Do they not wash?\n",
      "\n",
      "\n",
      "Arabs are dumb\n",
      "\n",
      "\n",
      "I dont hatefucking Islam\n",
      "\n",
      "\n",
      "I dont hate fucking Muslims\n",
      "\n",
      "\n",
      "Government is incompetent\n",
      "\n",
      "\n",
      "short women are fucking cute\n",
      "\n",
      "\n",
      "disabled people are stupid\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print sample of first 10 texts labeled \"correctly\" as hate:\n",
    "\n",
    "for t in df[cond1a & cond1b]['text'][:5]:\n",
    "    print(t)\n",
    "    print('\\n')\n",
    "    \n",
    "for t in df[cond2a & cond2b]['text'][:5]:\n",
    "    print(t)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dalits are lowlife scum\n",
      "\n",
      "\n",
      "i don't work this hard so that all those pakis can take the benefits\n",
      "\n",
      "\n",
      "i don't work this hard so that all those muslims can take the benefits\n",
      "\n",
      "\n",
      "why is it that africans smell weird? do they not wash?\n",
      "\n",
      "\n",
      "arabs are dumb\n",
      "\n",
      "\n",
      "i dont hatefucking islam\n",
      "\n",
      "\n",
      "i dont hate fucking muslims\n",
      "\n",
      "\n",
      "government is incompetent\n",
      "\n",
      "\n",
      "short women are fucking cute\n",
      "\n",
      "\n",
      "disabled people are stupid\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in df[cond1a & cond1b]['text'][:5]:\n",
    "    print(t)\n",
    "    print('\\n')\n",
    "    \n",
    "for t in df[cond2a & cond2b]['text'][:5]:\n",
    "    print(t)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "416263"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].apply(lambda x: len(x.split(' '))).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.text\n",
    "y = df.AL_found\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier for Multinomial Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
       "                ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6934865900383141\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.68      0.88      0.77      3016\n",
      "        True       0.72      0.44      0.55      2204\n",
      "\n",
      "    accuracy                           0.69      5220\n",
      "   macro avg       0.70      0.66      0.66      5220\n",
      "weighted avg       0.70      0.69      0.68      5220\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Support Vector Machine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
       "                ('clf',\n",
       "                 SGDClassifier(alpha=0.001, max_iter=5, random_state=42,\n",
       "                               tol=None))])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "               ])\n",
    "sgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6522988505747126\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.63      0.95      0.76      3016\n",
      "        True       0.78      0.24      0.37      2204\n",
      "\n",
      "    accuracy                           0.65      5220\n",
      "   macro avg       0.71      0.60      0.57      5220\n",
      "weighted avg       0.70      0.65      0.60      5220\n",
      "\n",
      "CPU times: user 84.9 ms, sys: 1.85 ms, total: 86.7 ms\n",
      "Wall time: 85.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "y_pred = sgd.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atsukosakurai/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
       "                ('clf', LogisticRegression(C=100000.0, n_jobs=1))])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "               ])\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6984674329501915\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.74      0.74      0.74      3016\n",
      "        True       0.64      0.64      0.64      2204\n",
      "\n",
      "    accuracy                           0.70      5220\n",
      "   macro avg       0.69      0.69      0.69      5220\n",
      "weighted avg       0.70      0.70      0.70      5220\n",
      "\n",
      "CPU times: user 86.7 ms, sys: 2.07 ms, total: 88.8 ms\n",
      "Wall time: 87.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.7 s, sys: 2.4 s, total: 34.1 s\n",
      "Wall time: 34.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "wv = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "wv.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Memorial_Hospital',\n",
       " 'Seniors',\n",
       " 'memorandum',\n",
       " 'elephant',\n",
       " 'Trump',\n",
       " 'Census',\n",
       " 'pilgrims',\n",
       " 'De',\n",
       " 'Dogs',\n",
       " '###-####_ext',\n",
       " 'chaotic',\n",
       " 'forgive',\n",
       " 'scholar',\n",
       " 'Lottery',\n",
       " 'decreasing',\n",
       " 'Supervisor',\n",
       " 'fundamentally',\n",
       " 'Fitness',\n",
       " 'abundance',\n",
       " 'Hold']"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import islice\n",
    "list(islice(wv.vocab, 13030, 13050))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        # FIXME: remove these examples in pre-processing\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, post) for post in text_list ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.3, random_state = 42)\n",
    "\n",
    "test_tokenized = test.apply(lambda r: w2v_tokenize_text(r['text']), axis=1).values\n",
    "train_tokenized = train.apply(lambda r: w2v_tokenize_text(r['text']), axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-165-664dfaa6d69c>:8: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  mean.append(wv.syn0norm[wv.vocab[word].index])\n",
      "WARNING:root:cannot compute similarity with no input ['ggggggggayyyyyyyyys', 'fiiiiiiiiiiiiiiltttttttttth']\n",
      "WARNING:root:cannot compute similarity with no input ['trrassssshhh', 'traaaaannns']\n",
      "WARNING:root:cannot compute similarity with no input ['h3sh3s', '4r3', '3rr0rs', 'of', 'n4tur3']\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input ['4r3']\n",
      "WARNING:root:cannot compute similarity with no input ['travellers', 'subhummmmmman']\n",
      "WARNING:root:cannot compute similarity with no input ['f4gg0ts', 'sh0uld']\n",
      "WARNING:root:cannot compute similarity with no input ['tr4nn13s', '4r3', 'n0t', 'n0rm4l']\n",
      "WARNING:root:cannot compute similarity with no input ['hefemales', 'areugly']\n",
      "WARNING:root:cannot compute similarity with no input ['ggggggggayyyyyyyyys', 'ffffffffilllllllllllthhhhhh']\n",
      "WARNING:root:cannot compute similarity with no input ['trrassssshhh', 'reeeeeeeefugeeeeeeee']\n",
      "WARNING:root:cannot compute similarity with no input ['g4iz', 'sh0uld']\n",
      "WARNING:root:cannot compute similarity with no input ['sp4ts', 'ar3', 'm0nk3ys']\n",
      "WARNING:root:cannot compute similarity with no input ['trrassssshhh', '=arrrabbbsss']\n",
      "WARNING:root:cannot compute similarity with no input ['trrassssshhh', 'immmmmmmigrrrrrant']\n",
      "WARNING:root:cannot compute similarity with no input ['diiiiiiirt', 'chhhhinese']\n",
      "WARNING:root:cannot compute similarity with no input ['trrrrrrrassssshhh', 'theblacks']\n",
      "WARNING:root:cannot compute similarity with no input ['trrrrrrrassssshhh', 'bbbblllllllllackkkkkkkks']\n",
      "WARNING:root:cannot compute similarity with no input ['wooooman', 'trrassssshhh']\n",
      "WARNING:root:cannot compute similarity with no input ['s1ck']\n",
      "WARNING:root:cannot compute similarity with no input ['w0m3n', '4r3', '4ll', 'sk4nks']\n",
      "WARNING:root:cannot compute similarity with no input ['dyk3s']\n",
      "WARNING:root:cannot compute similarity with no input ['dyk3s', 'sh0uld']\n",
      "WARNING:root:cannot compute similarity with no input ['terrrrrrrribbbbble', 'chhhhinese']\n",
      "WARNING:root:cannot compute similarity with no input ['trrassssshhh', 'chhhhiiiiineeeeese']\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input ['g4yz', 'sh0uld', 'd34d']\n",
      "WARNING:root:cannot compute similarity with no input ['trrrrrrrassssshhh', 'bbbbblaccccccccks']\n",
      "WARNING:root:cannot compute similarity with no input ['ggggggggayyyyyyyyys', 'freeeeeeeeeeeeeakz']\n",
      "WARNING:root:cannot compute similarity with no input ['f4uxb14ns', 'n33di', 'str41ghts']\n",
      "WARNING:root:cannot compute similarity with no input ['mmmmmusssssssslims', 'trrrrrrrrasssshhhhh']\n",
      "WARNING:root:cannot compute similarity with no input ['whit3s', 'ar3', 'canc3r']\n",
      "WARNING:root:cannot compute similarity with no input ['trrassssshhh', 'chhhhiiiiinesssse']\n",
      "WARNING:root:cannot compute similarity with no input ['w0m3n', 'ar3', 'th0ts']\n",
      "WARNING:root:cannot compute similarity with no input ['tr4nn13s', '4r3', '3rr0rs', 'of', 'n4tur3']\n",
      "WARNING:root:cannot compute similarity with no input ['f4uxb14ns', 'attention-seekers']\n",
      "WARNING:root:cannot compute similarity with no input ['diiiiiiiirty', 'chhhhinese']\n",
      "WARNING:root:cannot compute similarity with no input ['trrassssshhh', 'gayyyyyyyyyy']\n",
      "WARNING:root:cannot compute similarity with no input ['h3sh3s', '4r3', 'n0t', 'n0rm4l']\n",
      "WARNING:root:cannot compute similarity with no input ['woooooooooman', 'trrassssshhh']\n",
      "WARNING:root:cannot compute similarity with no input ['chhhhinese', 'diiiiiiiirty']\n",
      "WARNING:root:cannot compute similarity with no input ['1nd14ns', 'w0rse', 'th4n', 'c0ckr04ch3s']\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input ['chhhhinese', 'sccccccum']\n",
      "WARNING:root:cannot compute similarity with no input ['k3s']\n",
      "WARNING:root:cannot compute similarity with no input ['m3n', 'ar3', 'stup']\n",
      "WARNING:root:cannot compute similarity with no input ['buutttchhh', 'iiis', 'wr0000ngggg']\n",
      "WARNING:root:cannot compute similarity with no input ['trrassssshhh', 'lesssssssbiiiiiiiiiannnn']\n",
      "WARNING:root:cannot compute similarity with no input ['rohingya', 'maggotts']\n",
      "WARNING:root:cannot compute similarity with no input ['viiiiiiiile', 'chhhhinese']\n",
      "WARNING:root:cannot compute similarity with no input ['naaaaaaasty', 'chhhhinese']\n",
      "WARNING:root:cannot compute similarity with no input ['g4y', 'm3n', 'ar3', 'r4ts']\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input ['4r3']\n",
      "WARNING:root:cannot compute similarity with no input ['ggggggggayyyyyyyyys', 'subhummmmmman']\n",
      "WARNING:root:cannot compute similarity with no input ['terrrrrrrrible', 'chhhhinese']\n",
      "WARNING:root:cannot compute similarity with no input ['covid']\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input ['ggggggggayyyyyyyyys', 'dirrrrrrrrrrrtttttt']\n",
      "WARNING:root:cannot compute similarity with no input ['f4uxb14ns', 'n33dy', 'b4st4rds']\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input ['f4gg0tz', 'sh0uld', 'd34d']\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input ['4r4bs', 'n33di']\n",
      "WARNING:root:cannot compute similarity with no input ['jeeeeeewwwwwwwwssssss', 'cunnnnnnnnnnnnnnnttttttttttttts']\n",
      "WARNING:root:cannot compute similarity with no input ['4r3']\n",
      "WARNING:root:cannot compute similarity with no input ['wommennnnnn', 'freeeeeeeakz']\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input ['4r4bs', 'n33dy']\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input ['wommennnnnn', 'evvvvvvviiiiiiil']\n",
      "WARNING:root:cannot compute similarity with no input ['w0m3n', '4r3', '4ll', 'h4gs']\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input ['f4uxb14ns', 'n33dy']\n",
      "WARNING:root:cannot compute similarity with no input ['h0m0s', 'ar3', 'canc3r']\n",
      "WARNING:root:cannot compute similarity with no input ['mmmmmusssssslllllllllllllimmmmz', 'traaaaaaassssssssssh']\n",
      "WARNING:root:cannot compute similarity with no input ['trrrrrrrassssshhh', 'wwwwwwwommmmmmmen']\n",
      "WARNING:root:cannot compute similarity with no input ['chhhhinese', 'trrassssshhh']\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input ['chhhhinese', 'terrrrrrrribbbbble']\n",
      "WARNING:root:cannot compute similarity with no input ['trrassssshhh', 'chhhhinese']\n",
      "WARNING:root:cannot compute similarity with no input ['trrassssshhh', 'assssssian']\n",
      "WARNING:root:cannot compute similarity with no input ['k1k3z', 'wr0ng']\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input ['sccccccum', 'chhhhinese']\n",
      "WARNING:root:cannot compute similarity with no input ['p0l3s', 'd0gs']\n",
      "WARNING:root:cannot compute similarity with no input ['wooman', 'trrassssshhh']\n",
      "WARNING:root:cannot compute similarity with no input ['h3rm13s', '4r3', 'n0t', 'n0rm4l']\n",
      "WARNING:root:cannot compute similarity with no input []\n",
      "WARNING:root:cannot compute similarity with no input ['disableds', 'subhummmmmman']\n",
      "WARNING:root:cannot compute similarity with no input ['chhhhinese', 'viiiiiile']\n",
      "WARNING:root:cannot compute similarity with no input ['chhhhinese', 'diiiiiiirt']\n",
      "WARNING:root:cannot compute similarity with no input ['trrassssshhh', 'disssssssaaaaaaaableeeeeed']\n",
      "WARNING:root:cannot compute similarity with no input ['h3sh3s', '4r3', 's1ck']\n",
      "WARNING:root:cannot compute similarity with no input ['wommennnnnn', 'subhummmmmman']\n"
     ]
    }
   ],
   "source": [
    "X_train_word_average = word_averaging_list(wv,train_tokenized)\n",
    "X_test_word_average = word_averaging_list(wv,test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atsukosakurai/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg = logreg.fit(X_train_word_average, train['AL_found'])\n",
    "y_pred = logreg.predict(X_test_word_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6582375478927203\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.68      0.78      0.73      4545\n",
      "        True       0.62      0.49      0.54      3285\n",
      "\n",
      "    accuracy                           0.66      7830\n",
      "   macro avg       0.65      0.63      0.64      7830\n",
      "weighted avg       0.65      0.66      0.65      7830\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('accuracy %s' % accuracy_score(y_pred, test['AL_found']))\n",
    "print(classification_report(test['AL_found'], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2vec and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atsukosakurai/opt/anaconda3/lib/python3.8/site-packages/tqdm/std.py:697: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_sentences(corpus, label_type):\n",
    "    \"\"\"\n",
    "    Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n",
    "    We do this by using the TaggedDocument method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n",
    "    a dummy index of the post.\n",
    "    \"\"\"\n",
    "    labeled = []\n",
    "    for i, v in enumerate(corpus):\n",
    "        label = label_type + '_' + str(i)\n",
    "        labeled.append(TaggedDocument(v.split(), [label]))\n",
    "    return labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['AL_found'], random_state=0, test_size=0.3)\n",
    "X_train = label_sentences(X_train, 'Train')\n",
    "X_test = label_sentences(X_test, 'Test')\n",
    "all_data = X_train + X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['transvestites', 'are', 'wrong'], tags=['Train_0']),\n",
       " TaggedDocument(words=['these', 'low', 'iq', 'black', 'colonies,', 'their', 'racist', 'ideologues', 'and', 'their', 'masters', 'have', 'been', 'pushing', 'systematic', 'racism', 'against', '85%', 'native', 'white', 'population.'], tags=['Train_1'])]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26097/26097 [00:00<00:00, 2797095.84it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, min_count=1, alpha=0.065, min_alpha=0.065)\n",
    "model_dbow.build_vocab([x for x in tqdm(all_data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26097/26097 [00:00<00:00, 3500551.71it/s]\n",
      "100%|██████████| 26097/26097 [00:00<00:00, 4353794.66it/s]\n",
      "100%|██████████| 26097/26097 [00:00<00:00, 3744355.74it/s]\n",
      "100%|██████████| 26097/26097 [00:00<00:00, 3841736.33it/s]\n",
      "100%|██████████| 26097/26097 [00:00<00:00, 4533955.41it/s]\n",
      "100%|██████████| 26097/26097 [00:00<00:00, 3744740.04it/s]\n",
      "100%|██████████| 26097/26097 [00:00<00:00, 4142399.01it/s]\n",
      "100%|██████████| 26097/26097 [00:00<00:00, 4449723.63it/s]\n",
      "100%|██████████| 26097/26097 [00:00<00:00, 3743587.38it/s]\n",
      "100%|██████████| 26097/26097 [00:00<00:00, 3746919.23it/s]\n",
      "100%|██████████| 26097/26097 [00:00<00:00, 4320114.91it/s]\n",
      "100%|██████████| 26097/26097 [00:00<00:00, 4537902.72it/s]\n",
      "100%|██████████| 26097/26097 [00:00<00:00, 3767943.25it/s]\n",
      "100%|██████████| 26097/26097 [00:00<00:00, 3998347.15it/s]\n",
      "100%|██████████| 26097/26097 [00:00<00:00, 4510972.66it/s]\n",
      "100%|██████████| 26097/26097 [00:00<00:00, 3739878.07it/s]\n",
      "100%|██████████| 26097/26097 [00:00<00:00, 4131922.22it/s]\n",
      "100%|██████████| 26097/26097 [00:00<00:00, 4516743.07it/s]\n",
      "100%|██████████| 26097/26097 [00:00<00:00, 3352591.24it/s]\n",
      "100%|██████████| 26097/26097 [00:00<00:00, 4315686.29it/s]\n",
      "100%|██████████| 26097/26097 [00:00<00:00, 4444123.08it/s]\n",
      "100%|██████████| 26097/26097 [00:00<00:00, 3912175.26it/s]\n",
      "100%|██████████| 26097/26097 [00:00<00:00, 4230289.91it/s]\n",
      "100%|██████████| 26097/26097 [00:00<00:00, 3934393.14it/s]\n",
      "100%|██████████| 26097/26097 [00:00<00:00, 3694311.37it/s]\n",
      "100%|██████████| 26097/26097 [00:00<00:00, 3724480.30it/s]\n",
      "100%|██████████| 26097/26097 [00:00<00:00, 3723340.07it/s]\n",
      "100%|██████████| 26097/26097 [00:00<00:00, 3737196.61it/s]\n",
      "100%|██████████| 26097/26097 [00:00<00:00, 3702183.30it/s]\n",
      "100%|██████████| 26097/26097 [00:00<00:00, 3721694.31it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
    "    \"\"\"\n",
    "    Get vectors from trained doc2vec model\n",
    "    :param doc2vec_model: Trained Doc2Vec model\n",
    "    :param corpus_size: Size of the data\n",
    "    :param vectors_size: Size of the embedding vectors\n",
    "    :param vectors_type: Training or Testing vectors\n",
    "    :return: list of vectors\n",
    "    \"\"\"\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = vectors_type + '_' + str(i)\n",
    "        vectors[i] = model.docvecs[prefix]\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors_dbow = get_vectors(model_dbow, len(X_train), 300, 'Train')\n",
    "test_vectors_dbow = get_vectors(model_dbow, len(X_test), 300, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atsukosakurai/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000.0, n_jobs=1)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg.fit(train_vectors_dbow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atsukosakurai/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "logreg = logreg.fit(train_vectors_dbow, y_train)\n",
    "y_pred = logreg.predict(test_vectors_dbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6369093231162196\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.65      0.78      0.71      4504\n",
      "        True       0.60      0.44      0.51      3326\n",
      "\n",
      "    accuracy                           0.64      7830\n",
      "   macro avg       0.63      0.61      0.61      7830\n",
      "weighted avg       0.63      0.64      0.63      7830\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOW with keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tensorflow.keras import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 18267\n",
      "Test size: 7830\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(df) * .7)\n",
    "print (\"Train size: %d\" % train_size)\n",
    "print (\"Test size: %d\" % (len(df) - train_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_posts = df['text'][:train_size]\n",
    "train_tags = df['AL_found'][:train_size]\n",
    "\n",
    "test_posts = df['text'][train_size:]\n",
    "test_tags = df['AL_found'][train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 1000\n",
    "tokenize = text.Tokenizer(num_words=max_words, char_level=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize.fit_on_texts(train_posts) # only fit on train\n",
    "x_train = tokenize.texts_to_matrix(train_posts)\n",
    "x_test = tokenize.texts_to_matrix(test_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = np.max(y_train) + 1\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
